{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run my mini framework for Plotly visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%run ./PlotLee_Mini/setup_notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import ast\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import pprint\n",
    "import logging\n",
    "import re\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import swifter\n",
    "import itertools\n",
    "from geopy.geocoders import Nominatim\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    " \n",
    " \n",
    "# STOPWORDS = set(stopwords.words('english'))\n",
    "px.defaults.template = 'bnw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not logger.hasHandlers():      \n",
    "    filename = \"linkedin-job-data.log\"\n",
    "    ch = logging.FileHandler(filename)\n",
    "    ch.setLevel(logging.WARNING)\n",
    "    ## Handler also needs formatter\n",
    "    formatter = logging.Formatter('%(levelname)s -- %(asctime)s -- %(message)s')\n",
    "    # add formatter to ch\n",
    "    ch.setFormatter(formatter)\n",
    "    # add ch to logger\n",
    "    logger.addHandler(ch)\n",
    "    print(f\"Added File Handler {filename} to Logger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Save Stopwords to GBQ table if not already done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "stopwords_file = requests.get(\"https://gist.githubusercontent.com/sebleier/554280/raw/7e0e4a1ce04c2bb7bd41089c9821dbcf6d0c786c/NLTK's%2520list%2520of%2520english%2520stopwords\")\n",
    "STOPWORDS = str(stopwords_file.text).split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = [dataset.dataset_id for dataset in client.list_datasets()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./useful_code_functions/client_dataset_creation_functions.txt\", mode=\"w\") as f:\n",
    "#     f.write(str(type(client)) + \"\\n\")\n",
    "#     f.write(\"\\n\\t\".join([function for function in dir(client) if not function.startswith(\"_\")])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"helper\" in all_datasets:\n",
    "    print(\"helper already exists\")\n",
    "else:\n",
    "    client.create_dataset(dataset=\"helper\")\n",
    "    pd.DataFrame(STOPWORDS, columns=[\"stopword\"]).to_gbq(f\"helper.stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Exploration & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2023-09-26T17:27:28.793895Z",
     "iopub.status.busy": "2023-09-26T17:27:28.793468Z",
     "iopub.status.idle": "2023-09-26T17:27:28.923867Z",
     "shell.execute_reply": "2023-09-26T17:27:28.922820Z",
     "shell.execute_reply.started": "2023-09-26T17:27:28.793863Z"
    }
   },
   "outputs": [],
   "source": [
    "df_linkedin_listing_usa = pd.read_csv(\"./linkedin-data-analyst-jobs-listings-csv/linkedin-jobs-usa.csv\")\n",
    "df_linkedin_listing_usa[\"country\"] = \"USA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T17:28:18.641835Z",
     "iopub.status.busy": "2023-09-26T17:28:18.641388Z",
     "iopub.status.idle": "2023-09-26T17:28:18.910830Z",
     "shell.execute_reply": "2023-09-26T17:28:18.909545Z",
     "shell.execute_reply.started": "2023-09-26T17:28:18.641791Z"
    }
   },
   "outputs": [],
   "source": [
    "df_linkedin_listing_canada = pd.read_csv(\"./linkedin-data-analyst-jobs-listings-csv/linkedin-jobs-canada.csv\")\n",
    "df_linkedin_listing_canada[\"country\"] = \"Canada\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T17:31:26.028194Z",
     "iopub.status.busy": "2023-09-26T17:31:26.027706Z",
     "iopub.status.idle": "2023-09-26T17:31:26.033913Z",
     "shell.execute_reply": "2023-09-26T17:31:26.032768Z",
     "shell.execute_reply.started": "2023-09-26T17:31:26.028160Z"
    }
   },
   "outputs": [],
   "source": [
    "assert list(df_linkedin_listing_usa.columns) == list(df_linkedin_listing_canada.columns), \\\n",
    "\"Columns not equal, cannot concat vertically\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T17:31:56.099969Z",
     "iopub.status.busy": "2023-09-26T17:31:56.099553Z",
     "iopub.status.idle": "2023-09-26T17:31:56.106000Z",
     "shell.execute_reply": "2023-09-26T17:31:56.105070Z",
     "shell.execute_reply.started": "2023-09-26T17:31:56.099938Z"
    }
   },
   "outputs": [],
   "source": [
    "df_linkedin_listing = pd.concat([df_linkedin_listing_usa, df_linkedin_listing_canada])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-20T14:26:43.633495Z",
     "iopub.status.busy": "2023-09-20T14:26:43.632938Z",
     "iopub.status.idle": "2023-09-20T14:26:43.686580Z",
     "shell.execute_reply": "2023-09-20T14:26:43.684937Z",
     "shell.execute_reply.started": "2023-09-20T14:26:43.633444Z"
    }
   },
   "outputs": [],
   "source": [
    "df_linkedin_listing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-20T14:26:43.689894Z",
     "iopub.status.busy": "2023-09-20T14:26:43.688482Z",
     "iopub.status.idle": "2023-09-20T14:26:43.721773Z",
     "shell.execute_reply": "2023-09-20T14:26:43.720397Z",
     "shell.execute_reply.started": "2023-09-20T14:26:43.689815Z"
    }
   },
   "outputs": [],
   "source": [
    "df_linkedin_listing.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed that there are some unnormalized JSONs in the criteria column. I will normalize the criteria column and find out the nullity of the normalized criteria column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Normalization\n",
    "\n",
    "Let's normalize the criteria column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T17:34:27.139847Z",
     "iopub.status.busy": "2023-09-26T17:34:27.139443Z",
     "iopub.status.idle": "2023-09-26T17:34:27.408423Z",
     "shell.execute_reply": "2023-09-26T17:34:27.407070Z",
     "shell.execute_reply.started": "2023-09-26T17:34:27.139817Z"
    }
   },
   "outputs": [],
   "source": [
    "criteria_records = []\n",
    "for criteria_string_form in df_linkedin_listing[\"criteria\"]:\n",
    "    criteria_array = None\n",
    "    try:\n",
    "        criteria_array = ast.literal_eval(criteria_string_form)\n",
    "    except Exception as e:\n",
    "        logger.warn(\"%s %s\", e, criteria_string_form)\n",
    "        continue    \n",
    "    criteria_records.append({k:v for criteria_dict in criteria_array for k,v in criteria_dict.items()})\n",
    "\n",
    "# criteria_df = df_linkedin_listing.iloc[0:3].apply(convert_jsons_to_table, axis=1)\n",
    "criteria_records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T17:35:47.403801Z",
     "iopub.status.busy": "2023-09-26T17:35:47.403319Z",
     "iopub.status.idle": "2023-09-26T17:35:47.419504Z",
     "shell.execute_reply": "2023-09-26T17:35:47.418315Z",
     "shell.execute_reply.started": "2023-09-26T17:35:47.403767Z"
    }
   },
   "outputs": [],
   "source": [
    "criteria_df = pd.DataFrame.from_records(criteria_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-26T17:35:48.404350Z",
     "iopub.status.busy": "2023-09-26T17:35:48.403966Z",
     "iopub.status.idle": "2023-09-26T17:35:48.427205Z",
     "shell.execute_reply": "2023-09-26T17:35:48.425662Z",
     "shell.execute_reply.started": "2023-09-26T17:35:48.404322Z"
    }
   },
   "outputs": [],
   "source": [
    "# Validation Cell\n",
    "criteria_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2023-09-26T17:35:58.987412Z",
     "iopub.status.busy": "2023-09-26T17:35:58.986081Z",
     "iopub.status.idle": "2023-09-26T17:35:59.005471Z",
     "shell.execute_reply": "2023-09-26T17:35:59.004037Z",
     "shell.execute_reply.started": "2023-09-26T17:35:58.987366Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.merge(df_linkedin_listing, criteria_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T17:35:59.696190Z",
     "iopub.status.busy": "2023-09-26T17:35:59.695768Z",
     "iopub.status.idle": "2023-09-26T17:35:59.713753Z",
     "shell.execute_reply": "2023-09-26T17:35:59.712389Z",
     "shell.execute_reply.started": "2023-09-26T17:35:59.696159Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Let's drop the criteria column which holds the json that we have already normalized into other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T18:00:24.941224Z",
     "iopub.status.busy": "2023-09-26T18:00:24.940805Z",
     "iopub.status.idle": "2023-09-26T18:00:24.949006Z",
     "shell.execute_reply": "2023-09-26T18:00:24.947768Z",
     "shell.execute_reply.started": "2023-09-26T18:00:24.941189Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(\"criteria\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T18:00:30.438519Z",
     "iopub.status.busy": "2023-09-26T18:00:30.438075Z",
     "iopub.status.idle": "2023-09-26T18:00:30.475709Z",
     "shell.execute_reply": "2023-09-26T18:00:30.474647Z",
     "shell.execute_reply.started": "2023-09-26T18:00:30.438485Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Seniority Levels are \"Not Applicable\"\n",
    "\n",
    "Let's change those to np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Seniority level\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Seniority level\"] = df[\"Seniority level\"].replace(\"Not Applicable\", np.nan)\n",
    "df[\"Seniority level\"].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Job Descriptions are marked as the string 'Nan' instead of being an actual null value. \n",
    "Change those 'Nan' strings to np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[169, \"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace 'Nan' strings with np.nan\n",
    "df[\"description\"] = \\\n",
    "    df[\"description\"].replace(\"^(?i)nan$\", np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./linkedin-data-analyst-jobs-listings-csv/Linkedin_Jobs_American_and_Canadian.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Basic Distributions for Each Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T18:06:04.648686Z",
     "iopub.status.busy": "2023-09-26T18:06:04.648234Z",
     "iopub.status.idle": "2023-09-26T18:06:04.687547Z",
     "shell.execute_reply": "2023-09-26T18:06:04.686104Z",
     "shell.execute_reply.started": "2023-09-26T18:06:04.648649Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tallies = df[\"title\"].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_table = title_tallies.copy()\n",
    "titles_table[\"percent\"] = ((titles_table[\"title\"] / len(df) * 100).round(0)).astype(str) + \"%\"\n",
    "titles_table[\"plotly_text\"] = titles_table[\"index\"] + \"<br>\" + titles_table[\"percent\"] \n",
    "titles_table.loc[3:, \"plotly_text\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T18:06:06.495526Z",
     "iopub.status.busy": "2023-09-26T18:06:06.495140Z",
     "iopub.status.idle": "2023-09-26T18:06:08.747881Z",
     "shell.execute_reply": "2023-09-26T18:06:08.746815Z",
     "shell.execute_reply.started": "2023-09-26T18:06:06.495497Z"
    }
   },
   "outputs": [],
   "source": [
    "fig1 = px.pie(title_tallies, names=\"index\", values=\"title\", color=\"index\",\n",
    "             color_discrete_sequence=px.colors.qualitative.Pastel2)\n",
    "remove_px_attributes(fig1)\n",
    "fig1.update_traces(texttemplate=\"%{text}\", text=titles_table[\"plotly_text\"])\n",
    "fig1.add_annotation(text=f\"Sample Size: {sum(df['title'].isnull() == False)}\", xref=\"paper\", yref=\"paper\", xanchor=\"right\",\n",
    "                   yanchor=\"top\", x=1, y=-0.1, ax=0, ay=0)\n",
    "fig1.update_layout(\n",
    "    uniformtext_minsize=7, uniformtext_mode=\"hide\",\n",
    "    title=dict(\n",
    "    text=\"Types of Jobs in Dataset\",\n",
    "    x=0.05,\n",
    "    xanchor=\"left\"\n",
    "), margin=dict(t=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-20T06:45:28.738976Z",
     "iopub.status.busy": "2023-09-20T06:45:28.738514Z",
     "iopub.status.idle": "2023-09-20T06:45:28.748635Z",
     "shell.execute_reply": "2023-09-20T06:45:28.747493Z",
     "shell.execute_reply.started": "2023-09-20T06:45:28.738939Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"posted_date\"] = pd.to_datetime(df[\"posted_date\"], format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-20T06:55:46.253296Z",
     "iopub.status.busy": "2023-09-20T06:55:46.252878Z",
     "iopub.status.idle": "2023-09-20T06:55:46.408623Z",
     "shell.execute_reply": "2023-09-20T06:55:46.407492Z",
     "shell.execute_reply.started": "2023-09-20T06:55:46.253261Z"
    }
   },
   "outputs": [],
   "source": [
    "fig2 = px.line(df[\"posted_date\"].value_counts().sort_index())\n",
    "fig2.update_yaxes(title=\"Number of Jobs\")\n",
    "fig2.update_xaxes(title=\"Posted Date\")\n",
    "fig2.update_layout(title=\"Number of Jobs Over Time\", margin=dict(r=50))\n",
    "fig2.update_traces(hovertemplate=\"<b>Date:</b> %{x}<br><b>Number of Jobs:</b> %{y}<br>\", mode=\"lines+markers\")\n",
    "fig2.add_annotation(text=f\"Sample Size: {sum(~df['posted_date'].isnull())}\", xref=\"paper\", yref=\"paper\", xanchor=\"right\",\n",
    "                   yanchor=\"top\", x=1, y=-0.1, ax=0, ay=0)\n",
    "fig2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3 = px.histogram(df, x=\"onsite_remote\")\n",
    "remove_px_attributes(fig3)\n",
    "# add_percent_labels_for_hist(fig3, custom=True, custom_text_arr=df[\"onsite_remote\"].value_counts())\n",
    "add_annotation_for_figure(fig3, f\"Sample Size  {sum(df['onsite_remote'].isnull() == False)}\", \n",
    "                          x_anchor=\"right\", x=1)\n",
    "fig3.update_yaxes(title=dict(text=\"count\", standoff=20))\n",
    "fig3.update_xaxes(title=\"onsite_remote\")\n",
    "fig3.update_traces(text=df[\"onsite_remote\"].value_counts(sort=False))\n",
    "fig3.update_layout(title=\"Number of Jobs by Work Style\", margin=dict(l=100), coloraxis_showscale=False, bargap=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_counts = df[\"Industries\"].value_counts().reset_index()\n",
    "industries_table = industry_counts.copy()\n",
    "industries_table[\"percent\"] = ((industries_table[\"Industries\"] / len(df[~df[\"Industries\"].isnull()]) * 100).round(0)).astype(str) + \"%\"\n",
    "industries_table[\"plotly_text\"] = industries_table[\"index\"] + \"<br>\" + industries_table[\"percent\"] \n",
    "industries_table.loc[5:, \"plotly_text\"] = \"\"\n",
    "industries_table[\"plotly_text\"] = break_text(industries_table[\"plotly_text\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-20T06:47:54.945517Z",
     "iopub.status.busy": "2023-09-20T06:47:54.945088Z",
     "iopub.status.idle": "2023-09-20T06:48:15.584269Z",
     "shell.execute_reply": "2023-09-20T06:48:15.583460Z",
     "shell.execute_reply.started": "2023-09-20T06:47:54.945480Z"
    }
   },
   "outputs": [],
   "source": [
    "fig4 = px.pie(industry_counts, names=\"index\", values=\"Industries\",\n",
    "              color_discrete_sequence=px.colors.qualitative.Set3)\n",
    "remove_px_attributes(fig4)\n",
    "add_annotation_for_figure(fig4, f\"Sample Size:  {sum(df['Industries'].isnull() == False)}\",\n",
    "                          x_anchor=\"right\", x=1)\n",
    "fig4.update_layout(\n",
    "    uniformtext_minsize=12, uniformtext_mode=\"show\", title=dict(\n",
    "    text=\"Jobs by Sector\",\n",
    "    xanchor=\"left\",\n",
    "    x=0.05,\n",
    "    y=0.95,\n",
    "    ), margin=dict(t=120)\n",
    ")\n",
    "fig4.update_traces(texttemplate=\"%{text}\", text=industries_table[\"plotly_text\"])\n",
    "fig4\n",
    "\n",
    "# for i, trace in enumerate(fig4.data):\n",
    "#     print(trace[\"name\"])\n",
    "#     if not trace[\"name\"] in items_to_show:\n",
    "#         fig4.data[i][\"showlegend\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig5 = px.pie(df[\"Job function\"].value_counts().reset_index(), names=\"index\", values=\"Job function\",\n",
    "              color_discrete_sequence=px.colors.qualitative.Set3)\n",
    "remove_px_attributes(fig5)\n",
    "add_annotation_for_figure(fig5, f\"Sample Size:  {sum(df['Job function'].isnull() == False)}\",\n",
    "                          x_anchor=\"right\", x=1)\n",
    "fig5.update_layout(\n",
    "    uniformtext_minsize=12, uniformtext_mode=\"hide\", title=dict(\n",
    "    text=\"Jobs by Function\",\n",
    "    xanchor=\"left\",\n",
    "    x=0.05,\n",
    "    y=0.95)\n",
    ")\n",
    "fig5.update_traces(textposition=\"inside\", texttemplate=\"%{percent}\")\n",
    "fig5\n",
    "\n",
    "# for i, trace in enumerate(fig4.data):\n",
    "#     print(trace[\"name\"])\n",
    "#     if not trace[\"name\"] in items_to_show:\n",
    "#         fig4.data[i][\"showlegend\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# employment_counts = df[\"Employment type\"].value_counts(sort=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# employment_counts[\"percentage_decrease\"] = np.where(\n",
    "#     np.isnan(employment_counts[\"Employment type\"].shift(-1)),\n",
    "#     \"\",\n",
    "#     (\n",
    "#         (employment_counts[\"Employment type\"].shift(-1) - employment_counts[\"Employment type\"]) / employment_counts[\"Employment type\"] * 100\n",
    "#     ).round(2).astype(str) + \"%\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# employment_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig6 = px.histogram(df[~df[\"Employment type\"].isnull()], \n",
    "                    x=\"Employment type\")\n",
    "remove_px_attributes(fig6)\n",
    "# add_percent_labels_for_hist(fig6, custom=True, custom_text_arr=df[\"Employment type\"].value_counts().tolist())\n",
    "add_annotation_for_figure(fig6, f\"Jobs by Employment Type:  {sum(df['Employment type'].isnull() == False)}\", \n",
    "                          x_anchor=\"right\", x=1)\n",
    "fig6.update_traces(text=df[\"Employment type\"].value_counts(sort=False),\n",
    "                   marker_color=px.colors.qualitative.D3[1])\n",
    "fig6.update_yaxes(title=dict(text=\"count\", standoff=20))\n",
    "fig6.update_xaxes(title=\"Employment type\")\n",
    "fig6.update_layout(title=\"Number of Jobs by Employment Type\", margin=dict(l=100), coloraxis_showscale=False,\n",
    "                   bargap=0.4)\n",
    "\n",
    "# fig6.add_trace(go.Scatter(x=employment_counts[\"index\"], y=employment_counts[\"Employment type\"],\n",
    "#                           mode=\"lines+markers\"))\n",
    "\n",
    "# for rowNum, row in employment_counts.iterrows():\n",
    "#     if rowNum == len(employment_counts) - 1:\n",
    "#         continue\n",
    "\n",
    "#     y1 = row[\"Employment type\"]\n",
    "#     y2 = employment_counts.loc[rowNum + 1, \"Employment type\"]\n",
    "#     fig6.add_annotation(x=row[\"index\"], y=y1 - (y1 - y2) / 2, text=row[\"percentage_decrease\"], xshift=150, ax=0, ay=0)\n",
    "# fig6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[~df[\"salary\"].isnull(), \"salary\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salary data looks unclean so let's fix that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salary = pd.DataFrame()\n",
    "df_salary[\"salary_has_CA\"] = df[\"salary\"].replace(\"[\\r\\n\\s,]+\", \"\", regex=True).str.contains(\"CA\")\n",
    "df_salary[\"salary_text\"] = (\n",
    "    df[\"salary\"].replace(\"[\\r\\n\\s,]+\", \"\", regex=True)\n",
    "    .replace(\"CA\", \"\", regex=True) # replace spaces and unexpected text\n",
    ")\n",
    "\n",
    "salaries = df_salary[\"salary_text\"].str.split(\"-\")\n",
    "\n",
    "## Left bound and upper bound salaries\n",
    "df_salary[\"salary_lb\"] = salaries.str[0].str.strip(\"$\").astype(float)\n",
    "df_salary[\"salary_ub\"] = salaries.str[1].str.strip(\"$\").astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salary.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df_salary, x=\"salary_lb\")\n",
    "fig.update_layout(title=\"Distribution of Salary (Lower Bound Distribution)\")\n",
    "add_annotation_for_figure(fig, f\"Sample Size: {sum(~df_salary['salary_lb'].isnull())}\")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the salary values are bimodal, perhaps even trimodal. Some salaries are written as hourly salaries, while some appear to be monthly and other salaries appear to be annual.\n",
    "\n",
    "Let's **break down** the salary ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salary_hourly = df_salary.loc[df_salary[\"salary_lb\"] <= 2000] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salary_hourly.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salary.loc[(df_salary[\"salary_lb\"] >= 3000) & (df_salary[\"salary_lb\"] <= 7000)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it's safe the say that the salary date being provided is multimodal. Let's convert these salaries to annual salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_less_than_2000 = (df_salary[\"salary_lb\"] <= 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salary.loc[df_salary[\"salary_lb\"] <= 2000, [\"salary_lb\", \"salary_ub\"]]  = df_salary.loc[df_salary[\"salary_lb\"] <= 2000, [\"salary_lb\", \"salary_ub\"]] * 40 * 4 * 12  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salary.loc[(df_salary[\"salary_lb\"] >= 3000) & (df_salary[\"salary_lb\"] <= 7000) & ~previous_less_than_2000, [\"salary_lb\", \"salary_ub\"]] = \\\n",
    "df_salary.loc[(df_salary[\"salary_lb\"] >= 3000) & (df_salary[\"salary_lb\"] <= 7000) & ~previous_less_than_2000, [\"salary_lb\", \"salary_ub\"]] * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salary[\"salary_text\"] = np.where(\n",
    "    df_salary[\"salary_lb\"].isnull() & df_salary[\"salary_ub\"].isnull(), \n",
    "    np.nan, \n",
    "    \"$\" + df_salary[\"salary_lb\"].astype(str) + \"-\" + df_salary[\"salary_ub\"].astype(str) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distribution of salaries again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df_salary, x=\"salary_lb\")\n",
    "fig.update_layout(title=\"Distribution of Salary (Lower Bound Distribution)\")\n",
    "add_annotation_for_figure(fig, f\"Sample Size: {sum(~df_salary['salary_lb'].isnull())}\")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df_salary, x=\"salary_ub\")\n",
    "fig.update_layout(title=\"Distribution of Salary (Upper Bound Distribution)\")\n",
    "add_annotation_for_figure(fig, f\"Sample Size: {sum(~df_salary['salary_ub'].isnull())}\")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salary_analysis = df.merge(df_salary, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a conversion rate on salary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion Rate\n",
    "df_salary_analysis[\"salary_ub\"] = np.where(df_salary_analysis[\"country\"] == \"USA\", df_salary_analysis[\"salary_ub\"] * 1.33, df_salary_analysis[\"salary_ub\"])\n",
    "df_salary_analysis[\"salary_lb\"] = np.where(df_salary_analysis[\"country\"] == \"USA\", df_salary_analysis[\"salary_lb\"] * 1.33, df_salary_analysis[\"salary_lb\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert some ordinal values from strings to integers to analyze correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salary_analysis[\"Seniority level\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salary_analysis[\"Employment type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salary_analysis.groupby(\"Seniority level\").agg({\"salary_ub\": \"mean\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_df = df_salary_analysis.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_df[\"Seniority level\"].replace({\"Entry level\" : 1, \"Associate\" : 2, \"Mid-Senior level\" : 3, \"Executive\" : 4}, inplace=True)\n",
    "encode_df[\"Employment type\"].replace({\"Volunteer\" : 1, \"Temporary\" : 2, \"Contract\" : 3, \"Full-time\" : 4}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(encode_df.corr(), color_continuous_scale=\"BuGn\")\n",
    "fig.update_layout(title=dict(text=\"Correlations Between Ordinal Job Attributes\", x=0.05, y=0.96, xanchor=\"left\", font_size=24),\n",
    "                  margin=dict(b=160))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:**\n",
    "- Salary has a somewhat strong correlation with seniority level. This is expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig7 = px.box(df_salary_analysis, x=\"salary_lb\", color=\"Seniority level\")\n",
    "remove_px_attributes(fig7)\n",
    "add_annotation_for_figure(fig7, f\"Sample Size: {min(sum(~df_salary_analysis['salary_lb'].isnull()), sum(~df_salary_analysis['Seniority level'].isnull()))}\",\n",
    "                          x_anchor=\"center\", x=1.1)\n",
    "fig7.update_layout(showlegend=True, title=\"Salary (Lower Bound) Based on Seniority Level (in CAD)\")\n",
    "fig7.update_xaxes(showgrid=True)\n",
    "fig7.update_traces(hoverinfo=\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig8 = px.histogram(df_salary_analysis, x=\"Seniority level\", color=\"Seniority level\")\n",
    "remove_px_attributes(fig8)\n",
    "add_percent_labels_for_hist(fig8, custom=True, custom_text_arr=df_salary_analysis[\"Seniority level\"].value_counts(sort=False))\n",
    "add_annotation_for_figure(fig8, f\"Sample Size: {min(sum(~df_salary_analysis['salary_lb'].isnull()), sum(~df_salary_analysis['Seniority level'].isnull()))}\",\n",
    "                          x_anchor=\"center\", x=1)\n",
    "fig8.update_layout(showlegend=True, title=\"Jobs by Seniority Level\", bargap=0.5)\n",
    "fig8.update_xaxes(showgrid=True)\n",
    "fig8.update_traces(hoverinfo=\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Out Jobs Tables to Bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salary_analysis = df_salary_analysis.loc[:, df_salary_analysis.columns[~df_salary_analysis.columns.isin([\"link\", \"salary\"])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salary_analysis = \\\n",
    "(\n",
    "    df_salary_analysis.reset_index()\n",
    "                      .rename(columns={\n",
    "                          \"Seniority level\" : \"seniority_level\", \n",
    "                          \"Employment type\" : \"employment_type\", \n",
    "                          \"Job function\": \"job_function\", \n",
    "                          \"Industries\" : \"industries\",\n",
    "                          \"index\" : \"job_id\"\n",
    "                       })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salary_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salary_analysis.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DATASET_NAME and TABLE_NAME from .env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()\n",
    "DATASET_NAME = os.getenv('DATASET_NAME')\n",
    "TABLE_NAME = os.getenv('TABLE_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out table to GBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salary_analysis.to_gbq(f\"{DATASET_NAME}.{TABLE_NAME}\", if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Our Linkedin Jobs Exploration Dashboard\n",
    "\n",
    "To extract this report, open up a jupyter notebook server and save the report cell as an **Embed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import HBox, VBox\n",
    "figures = [fig1, fig4, fig5, fig2, fig3, fig6, fig7]\n",
    "jup_figures = convert_figures_to_figurewidgets(figures)\n",
    "pie_charts = VBox([jup_figures[0], jup_figures[1], jup_figures[2]])\n",
    "bar_charts = VBox([jup_figures[4], jup_figures[5]])\n",
    "time_chart = jup_figures[3]\n",
    "salary_chart_by_seniority = jup_figures[6]\n",
    "report = VBox([pie_charts, bar_charts, time_chart, salary_chart_by_seniority], layout=dict(height=\"3500px\", margin=\"0px 0px 0px 0px\", padding=\"0px 0px 0px 0px\",\n",
    "                                                                                           justify_content=\"space-between\", background=\"#f0f0f0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb Exploration\n",
    "\n",
    "Let's use Wandb to explore individual data rows easier\n",
    "\n",
    "Credit to https://www.kaggle.com/code/ayuraj/interactive-eda-using-w-b-tables/notebook for code and inspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import wandb\n",
    "# import wandb\n",
    "\n",
    "# try:\n",
    "#     secret_value_0 = os.getenv(\"WANDB_API\")\n",
    "#     wandb.login(key=secret_value_0)\n",
    "    \n",
    "#     anony=None\n",
    "# except Exception as e:\n",
    "#     anony = \"must\"\n",
    "#     print('If you want to use your W&B account, go to Add-ons -> Secrets and add your W&B access token. Use the Label name as \"wandb_api\". \\nGet your W&B access token from here: https://wandb.ai/authorize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = wandb.init(project='eda', anonymous=None) # W&B Code 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize a W&B run to log images\n",
    "# data_at = wandb.Table(columns=df_salary_analysis.columns.tolist()) # W&B Code 2\n",
    "# for i in tqdm(range(len(df_salary_analysis))):\n",
    "#     row = df_salary_analysis.loc[i]\n",
    "#     data_at.add_data(*tuple(row.values[0:])) # W&B Code 3\n",
    "\n",
    "# wandb.log({'LinkedIn Job Data': data_at}) # W&B Code 4\n",
    "# wandb.finish() # W&B Code 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract Entities From Job Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few entities I want to extract from job descriptions: \n",
    "- **Programming Languages**: I want to see what programming languages companies are interested in\n",
    "- **Soft Skills**:  I want to see what soft skills are expected of Data Analyst Jobs\n",
    "- **Hard Skills**: I want to understand what hard skills are expected of Data Analyst Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Use Spacy's Pretrained LLM Model to Extract Entitiess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze | grep spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordsegment is a library that probabilistically separates an undelimited piece of text. For example, it turns 'hiimjohn' into 'hi im john'\n",
    "# We may either use/not use wordsegment for entity extraction\n",
    "from wordsegment import load as load_words, segment, clean as segment_clean, WORDS, BIGRAMS, UNIGRAMS\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_words()\n",
    "len(BIGRAMS), len(UNIGRAMS), len(WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "setofwords = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description_example = nlp(df.loc[500, \"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.serve(job_description_example, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_named_entities(row, test=False, segment_sentence=False):\n",
    "    '''\n",
    "        Assumption: Segment every word\n",
    "    '''\n",
    "    job_description = row[\"description\"] if test is False else row\n",
    "\n",
    "    # Skip np.nan\n",
    "    if isinstance(job_description, float):\n",
    "        return []\n",
    "        \n",
    "    nlp_tokens = nlp(job_description)\n",
    "    all_entities = [[token.text, token.label_] for token in nlp_tokens.ents \n",
    "                    if token.text.lower() not in STOPWORDS]\n",
    "    \n",
    "    final_entities = []\n",
    "    if segment_sentence is True:\n",
    "        # Load \n",
    "        for ent_text, ent_label in all_entities:\n",
    "            try:\n",
    "                    is_entity_found = False\n",
    "                    texts = segment(ent_text) \n",
    "                    for text in texts:\n",
    "                        if text.lower() in UNIGRAMS and \\\n",
    "                            text.lower() in ENTITIES:\n",
    "                            final_entities.append(text)\n",
    "                            is_entity_found = True\n",
    "\n",
    "                    if is_entity_found is False:\n",
    "                        final_entities.append(ent_text)\n",
    "            except ValueError as e:\n",
    "                logger.warn(\"Unexpected text segmentation error: \", e)\n",
    "                final_entities.append(ent_text)\n",
    "    else:\n",
    "        final_entities = all_entities\n",
    "        \n",
    "    # np_final_word_array = np.expand_dims(np.array(all_words), axis=1)\n",
    "    # np_job_description_index = np.expand_dims([row.name if test is False else 0] \n",
    "    #                                           * len(all_words), axis=1)\n",
    "    # return np.concatenate((np_job_description_index, np_final_word_array), axis=1).tolist()\n",
    "    return final_entities\n",
    "\n",
    "all_entities = df.swifter.apply(find_all_named_entities, axis=1).explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entities.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess List of Entities derived from Spacy LLM\n",
    "entity_frame = pd.DataFrame(all_entities)\n",
    "entity_frame[\"entity\"] = entity_frame[0].str[0]\n",
    "entity_frame[\"type\"] = entity_frame[0].str[1]\n",
    "entity_frame.drop(columns=[0], inplace=True)\n",
    "entity_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_frame[\"type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_value_counts = []\n",
    "for entity_type in entity_frame[\"type\"].unique():\n",
    "    store_value_counts.append(\n",
    "        entity_frame[entity_frame[\"type\"] == entity_type].value_counts()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(store_value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_value_counts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_value_counts[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that clearly spacy's LLM is not trained to identify Data Analyst skills. However we know that utilizing GPT-NeoX or any Generative Autoregressive Model to extract entities with few-shot learning is not feasible for a personal project, as they cost way too much for reasonable entity extraction across many tokens. So I will settle with using what I am given: Spacy's Pre-trained LLMs and use their pre-defined entities to extract insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_frame = entity_frame[entity_frame[\"type\"].isin([\n",
    "    \"ORG\", \"LOC\", \"PRODUCT\", \"PERSON\", \"GPE\", \"LAW\", \"LANGUAGE\"\n",
    "])].reset_index().rename(columns={\"index\" : \"job_id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Write Out our Entities to GBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_frame.to_gbq(f\"{DATASET_NAME}.job_entities\", if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Try out nlpcloud models for Automatic Entity Extraction based on human input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpcloud\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Try llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = nlpcloud.Client(\"finetuned-llama-2-70b\", \"fbc5fd9ec4e8d9a0f42bb8e49edf7e84e2c639fe\",\n",
    "                        gpu=True, lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entities = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_words = df_salary_analysis.loc[0, \"description\"].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, len(split_words), CHUNK_SIZE):\n",
    "#     # Stay under llama's 256 token limit\n",
    "#     string_to_get_entities = \" \".join(split_words[i:i+CHUNK_SIZE])\n",
    "#     entity_dict = llama.entities(string_to_get_entities, searched_entity=\"hard skill\")\n",
    "#     all_entities.extend(entity_dict[\"entities\"])\n",
    "#     time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rate limiting is a problem for us. I do not wish to upgrade my free plan, so either I live with entity extraction under the free plan or I ditch the idea to use these powerful generative autoregressive LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama.entities(\"I want to use Tableau and be good at Python\", searched_entity=\"hard skill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extract Words From Job Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = nlp(\"I am singing in the garden\")\n",
    "# print([token.lemma_.lower() for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_words_based_on_spacy_delim(row, test=False, segment_sentence=False):\n",
    "    '''\n",
    "        Assumption: Segment every word\n",
    "    '''\n",
    "    job_description = row[\"description\"] if test is False else row\n",
    "    if isinstance(job_description, float):\n",
    "         return []\n",
    "    \n",
    "    # Get rid of dangling punctuations. Keep periods, commas, and semicolons. \n",
    "    # Unfortunately can't keep track of newline separators because of poor web scraping practices \n",
    "    all_words = re.split(r\"[!\\\"'#$%&'*+/:<=>?@^_`|~()[\\]{}\\s]+\", job_description)\n",
    "    final_words_array = []\n",
    "\n",
    "    if segment_sentence is False:\n",
    "        for word in all_words:\n",
    "            # If word's punctuation has position, then split. \n",
    "            # Assumption is that these punctuations exist at end of sentences.\n",
    "            if re.search(\"[,;.]+\", word):\n",
    "                # print(word)\n",
    "                start = re.search(\"[,;.]+\", word).start(0)\n",
    "                final_words_array.append(word[:start])\n",
    "                final_words_array.append(word[start:start+1])\n",
    "            else:\n",
    "                final_words_array.append(word)\n",
    "            # if not, then just add word to array\n",
    "    else:      \n",
    "        for word in all_words:\n",
    "                try:\n",
    "                    words = segment(word) ## ValueError can occur due to unavailable segmentation\n",
    "                    final_words_array.extend(words)\n",
    "                except ValueError:\n",
    "                    final_words_array.append(word)\n",
    "    \n",
    "    return final_words_array\n",
    "        \n",
    "\n",
    "all_words = df.swifter.apply(find_all_words_based_on_spacy_delim, axis=1).explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0, \"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"useful_code_functions/named_entity_methods.txt\", mode=\"w\") as f:\n",
    "#     f.write(str(type(a)))\n",
    "#     f.write(\"\\n\")\n",
    "#     f.write(str(type(a.ents[0])))\n",
    "#     f.write(\"\\n\".join(\n",
    "#         [\"\\t\" + elem for elem in dir(a.ents[0]) if not elem.startswith(\"_\")]\n",
    "#         )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes around 10 minutes to run machine learning model on ~5000 description items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We actually decided to keep punctuations for easier bigram and trigram analysis\n",
    "# import string\n",
    "# all_words_no_puncutation = all_words[~all_words.str.match(f\"^[{re.escape(string.punctuation)}\\s]+$\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_words = all_words.reset_index()\n",
    "final_words.columns = [\"job_id\", \"word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0, \"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_words[\"word\"] = final_words[\"word\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_words = final_words[final_words[\"word\"] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_words[\"row_num\"] = range(len(final_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_words.to_gbq(\"static_job_warehouse.job_words\", if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_based_on_delim = pd.DataFrame({\"Label_ID\": all_words.str[0], \n",
    "#                                     \"Word\" : all_words.str[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_based_on_delim.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_id_1843 = final_words[final_words[\"job_id\"] == 1843]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(job_id_1843)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_id_1843.iloc[400:440]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Testing between pd.Series.apply and pd.DataFrame.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "def get_keywords(row):\n",
    "    job_desc = row[\"description\"].lower()\n",
    "    has_qualification = job_desc.find(\"qualification\")\n",
    "    has_requirement = job_desc.find(\"requirement\")\n",
    "    has_basic_requirement = job_desc.find(\"basic requirement\")\n",
    "    has_responsibility = job_desc.find(\"responsibility\")\n",
    "    return [has_qualification, has_requirement, has_basic_requirement, has_responsibility]\n",
    "\n",
    "new_data = df.apply(get_keywords, result_type=\"expand\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = df.apply(get_keywords, result_type=\"expand\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_series(job_desc):\n",
    "    job_desc = job_desc.lower()\n",
    "    has_qualification = job_desc.find(\"qualification\")\n",
    "    has_requirement = job_desc.find(\"requirement\")\n",
    "    has_basic_requirement = job_desc.find(\"basic requirement\")\n",
    "    has_responsibility = job_desc.find(\"responsibilit\")\n",
    "    return [has_qualification, has_requirement, has_basic_requirement, has_responsibility]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "series = df[\"description\"].apply(get_keywords_series)\n",
    "new_data_series = pd.DataFrame(series.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = df[\"description\"].apply(get_keywords_series)\n",
    "new_data_series = pd.DataFrame(series.tolist(), columns=[\"has_qualification\", \"has_requirement\",\n",
    "                                                        \"has_basic_requirement\", \"has_responsibility\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_series.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "word_tokens = word_tokenize(example_sent)\n",
    "# converts the words in word_tokens to lower case and then checks whether\n",
    "# they are present in stop_words or not\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "#with no lower case conversion\n",
    "filtered_sentence = []\n",
    " \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    " \n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Test End\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
