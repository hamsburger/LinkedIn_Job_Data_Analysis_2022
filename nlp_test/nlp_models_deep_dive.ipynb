{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract Entities From Job Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few entities I want to extract from job descriptions: \n",
    "- **Programming Languages**: I want to see what programming languages companies are interested in\n",
    "- **Soft Skills**:  I want to see what soft skills are expected of Data Analyst Jobs\n",
    "- **Hard Skills**: I want to understand what hard skills are expected of Data Analyst Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Spacy Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze | grep spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.0/en_core_web_sm-3.7.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 8.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from en-core-web-sm==3.7.0) (3.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (65.6.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.4.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.10.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.22.0)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (8.2.1)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.3.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (4.66.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (23.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (4.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (8.1.3)\n",
      "Requirement already satisfied: cloudpathlib<0.16.0,>=0.7.0 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\harri\\miniconda3\\envs\\py310_notebook_env\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.1.1)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordsegment is a library that probabilistically separates an undelimited piece of text. For example, it turns 'hiimjohn' into 'hi im john'\n",
    "# We may either use/not use wordsegment for entity extraction\n",
    "from wordsegment import load as load_words, segment, clean as segment_clean, WORDS, BIGRAMS, UNIGRAMS\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(258437, 333213, 178758)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_words()\n",
    "len(BIGRAMS), len(UNIGRAMS), len(WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "setofwords = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Test out Doc and Span classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "# Create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "# Add span to the doc.ents\n",
    "doc.ents = [span_with_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents[0].start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'appositional modifier'"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"appos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Spacy Custom Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'animal_component']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", animal_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component_function(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches =  matcher(doc)\n",
    "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe(\"animal_component\", after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rate limiting is a problem for us. I do not wish to upgrade my free plan, so either I live with entity extraction under the free plan or I ditch the idea to use these powerful generative autoregressive LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': [{'start': 14,\n",
       "   'end': 21,\n",
       "   'type': 'hard skill',\n",
       "   'text': 'Tableau'},\n",
       "  {'start': 37, 'end': 43, 'type': 'hard skill', 'text': 'Python'}]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama.entities(\"I want to use Tableau and be good at Python\", searched_entity=\"hard skill\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
